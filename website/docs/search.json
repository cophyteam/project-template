[
  {
    "objectID": "Tutorial_Cluster_Part2.html",
    "href": "Tutorial_Cluster_Part2.html",
    "title": "Part 2: Good practices",
    "section": "",
    "text": "If you managed to complete the first part of this tutorial, you will also be able to pip install whatever in your virtual environment and do some computing. However, there is more to know."
  },
  {
    "objectID": "Tutorial_Cluster_Part2.html#sharing-the-resources",
    "href": "Tutorial_Cluster_Part2.html#sharing-the-resources",
    "title": "Part 2: Good practices",
    "section": "Sharing the resources",
    "text": "Sharing the resources\nBecause it needs to remain highly flexible and adapted to a wide range of needs, the cluster is not very constrained with respect to resource allocation.\nIf you do not pay attention, you might monopolize all the CPUs or all the memory with your jobs, without leaving anything behind for your colleagues.\nThat why evaluating the amount of memory you need and the maximum time that a non-bugged job might take is important! Based on this information, you can adjust mem_gb and timeout_min (timeout in minutes) well.\nSimilarly, you may need to decide how many CPUs will be useful for you. Can you go with only one without losing much? Then use only 1. Do you divide your computation time by a huge factor if you use more, then use more. But how will you know?\nWhat follows should help you with all this.\n\nAnticipating time-memory consumption\nHereafter, we use memory_usage, which has a slighty unusual way of passing arguments to its target function. All positional arguments (those without an = sign in the def) are passed together, and all non-positional arguments (also called key-pairs) are passed together. For example, we could try: mem_usage=memory_usage((somefunc,(0.1,4,0.88), {'file' : 'whatever.csv','index' : 0 }))  If we had a function defined like this:  somefunc(a,b,c, file=None, index=-1)\n\n###### simple memory/time check\nfrom memory_profiler import memory_usage\nimport time\n\n# define a single thread function\ndef duplicate_ones(a, n=100, x=0):\n    import time\n    time.sleep(1)\n    b = [a] * n\n    b = [a] * n\n    b = [a] * n\n    time.sleep(1)\n    return b\n\n# duplicate ones a million time\nprint('Duplicate ones a thousand times')\nstart_time = time.time()\nmem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e3)}))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\n# duplicate ones 100 million times\nprint('Duplicate ones a million time')\nstart_time = time.time()\nmem_usage=memory_usage((duplicate_ones,(1,), {'n' : int(1e8)}))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\nprint('Do you notice the difference in time and memory due to the change in duplication size?')\n\n\n\nEvaluating CPU count needs\nHow to evaluate whether our job will benefit from having more CPU available to them? If you don’t know whether your function use parallelization or not, because you relies on high-level toolboxes, then you can evaluate that empirically by looking at the time your jobs take depending on the number of CPUs you allow.\nLet’s try first with our last function. It should take about 10s to run.\n\nimport os \nimport submitit\n\n# these commands may not be necessary but helped overcoming an error initially\nos.environ['SLURM_CPUS_PER_TASK'] = str(1)\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n    \n# cpu counts to test\nnCPUs_totest=[1, 4]\n\n# loop over cpu counts\njcount=0\njoblist=[]\nstart_time = time.time()\nfor i, cpus in enumerate(nCPUs_totest):\n    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n    job = executor.submit(duplicate_ones, 1, int(1e8))\n    job.n_cpus=cpus\n    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n    joblist.append(job)\n    jcount=jcount+1\n\n# wait for job completion\nnjobs_finished = sum(job.done() for job in joblist)\nwhile njobs_finished&lt;jcount:\n    doneIdx=-1\n    time.sleep(1)\n    for j, job in enumerate(joblist):\n        if job.done():\n            doneIdx=j\n            break\n    if doneIdx&gt;=0:\n        print(str(njobs_finished)+' on ' + str(jcount))\n        # report last job finished\n        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n        joblist.pop(doneIdx)\n        njobs_finished=njobs_finished+1\n\nprint('### Do you think that increasing the number of CPUs made a big difference? ###')\n\nNow let’s redo exactly the same thing, with with a numpy function may benefit from multiple CPUs (i.e. np.dot).\n\nimport numpy as np\nimport time\n\ndef mat_multiply(size):\n  # Generate large random matrices\n  A = np.random.rand(size, size)\n  B = np.random.rand(size, size)\n\n  # Measure time for matrix multiplication\n  C = np.dot(A, B)\n  \n  return 'this function does not return anything special'\n  \nos.environ['SLURM_CPUS_PER_TASK'] = str(1)\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n\n# cpu counts to test\nnCPUs_totest=[4, 4, 4, 1]\n\n# define the max number of jobs that may run in parallel\nmaxjobs=2\n\n# loop over cpu counts\njcount=0\njoblist=[]\nstart_time = time.time()\nfor i, cpus in enumerate(nCPUs_totest):\n    executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n    executor.update_parameters(mem_gb=4, timeout_min=5, slurm_partition=\"CPU\", cpus_per_task=cpus)\n    # check how many job are running (not done) and wait it they exceed our limit\n    while sum(not job.done() for job in joblist)&gt;maxjobs:\n        print('wait to submit new job')\n        time.sleep(3)\n    job = executor.submit(mat_multiply, 8000)\n    time.sleep(0.5)\n    job.n_cpus=cpus\n    print(\"job with \" + str(job.n_cpus) + \" cpus submitted\")\n    joblist.append(job)\n    jcount=jcount+1\n\n# wait for job completion\nnjobs_finished = 0; \nwhile njobs_finished&lt;jcount:\n    doneIdx=-1\n    time.sleep(1)\n    for j, job in enumerate(joblist):\n        if job.done():\n            doneIdx=j\n            break\n    if doneIdx&gt;=0:\n        print(str(njobs_finished)+' on ' + str(jcount))\n        # report last job finished and print stats\n        print(\"job with \" + str(job.n_cpus) + \" cpus returned in \" + str(time.time()-start_time) + \" seconds\")\n        print(\"job status: \" + job.state)\n        joblist.pop(doneIdx)\n        njobs_finished=njobs_finished+1\n\nprint('\\n### Do you think that increasing the number of CPUs made a big difference? ###')\nprint('\\n### MaxRSS indicates the memory used ###')\n\n\n\nScaling up responsibly\nIn the loop above, you might have noticed something new: we’ve implemented another good practice by self-limiting the number of jobs we will run in parallel on the cluster. Indeed, it might be ok to launch 40 or even 100 parallel jobs if you are in a hurry, but the amount of CPUs in the cluster is not infinite, and neither is the amount of memory.\nNumber of CPUs: you can get this information by running sinfo -o%C in your terminal, or !sinfo -o%C in the notebook. The CPU partitions have about 350 cores available at the time of writing Amount of memory: you can see this by running sinfo -o \"%P %n %m\" in your terminal (or with a ! in the notebook). The CPU partitions have about 2.3TB of memory at the time of writing.\nIf it is a sunday and nobody is using the cluster, it is probably fine to increase maxjobs to 100 or more (note that if you require 4 cpu per task, it means that you are actually requiring 400 cpus overall!). But if it is 10.30pm on a tuesday, using this parameter might be the same as walking to the coffee machine and taking all the coffee reserves to your office! So, take the habit of setting your maxjobs-like parameter on a daily basis after checking sinfo -o%C.\n\n# check node and CPU information\nprint(\"### Node counts: \\nA: currently in use \\B available\")\n!sinfo -o%A\nprint(\"### CPU counts: \\nA: core you currently use (notebook) \\nI: available \\nO: unavailable (maintenance, down, etc) \\nT: total\")\n!sinfo -o%C\n\n# check some stats of our last job\nprint('### CPU time and MaxRSS of our last job (about 1Gb should be added to your MaxRSS in order to cover safely the memory needs of the python runtime)###')\nos.system(f'sacct -j {job.job_id} --format=\"CPUTime,MaxRSS\"')\n\n\n\nA more compact approach\nIn the above examples, we have decomposed most operations using for loops in order to illustrate the different concepts. But with more advanced methods we can compact a lot the code used above.\nThe example below (taken from submitit documentation) allows getting rid of the job submission loop and directly map our input arrays to job submissions, using executor.map_array and some asynchronous operations. Note that such compact approach might be more difficult to debug.\n\nimport asyncio\n\n# just add a/b, multiply by c and wait for b seconds\ndef simple_function(a, b, c):\n    output=(a + b)*c\n    time.sleep(b)\n    return output\n\n# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\na = [1, 2, 2, 1, 0, 1]\nb = [10, 20, 30, 40, 30, 10]\nc=[0.1]*len(b)\n\n# make sure our arrays are matched in length\nassert len(a)==len(b)==len(c)\n\n# prepare executor\nexecutor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n\n# define maxjobs to a low value to illustrate\nmaxjobs=3\n\n# the pupdate_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n\n# execute the job (note the .map_array command that different from the .submit command used above)\njobs = executor.map_array(simple_function, a, b, c)  # just a list of jobs\n\n# print results as they become available\nfor aws in asyncio.as_completed([j.awaitable().result() for j in jobs]):\n    result = await aws\n    print(\"result of computation: \" + str(result))\n    arameter \"slurm_array_parallelism\" tells submitit to limit the number of concurrent jobs\nexecutor.\n# note that we use here an asynchronous method based on asyncio\n# it essential do something similar to what we were doing after \n# \"# wait for job completion\", but in a much more compact way\n# however, the reordering of outputs wrt to inputs is not implemented\n\n\n\nSubmitting and going home\nOften, when we have very long jobs, we want to submit these jobs, go home and come back the next day or the next week to check the results of their computations.\nIn this case, we should not expect our notebook to be still alive when we come back. Instead, we should adopt the more standard approach of writing down our results and load them in a new jupyter session afterwards!\nThis is way will we simulate in the final example below.\n\n# write in job_output within our home directory (~ is synonymous of /home/username/)\njob_output_folder=os.getcwd()+'/tuto_output/'\n\n# make sure our output folder exists\nif not os.path.exists(job_output_folder):\n  os.makedirs(job_output_folder)\n\n# just add a/b, multiply by c, wait for b seconds and write down the result to an output folder (c)\ndef simple_function_write(a, b, c):\n    output=(a + b)\n    time.sleep(b)\n    output_filepath=os.path.join(c, str(a) + '_' + str(b) + '.txt')\n    with open(output_filepath, 'w') as file:\n      file.write(f'{a}\\n')\n      file.write(f'{b}\\n')\n    \n# define arrays matched in length for the iteration (if you have constant parameters, you can always duplicate them as done with \"c\" below)\na = [1, 2, 2, 1, 0, 1]\nb = [10, 20, 30, 40, 30, 10]\nc=[job_output_folder]*len(b)\n\n# make sure our arrays are matched in length\nassert len(a)==len(b)==len(c)\n\n# prepare executor\nexecutor = submitit.AutoExecutor(folder=\"joblogs\")\n\n# define maxjobs to a low value to illustrate\nmaxjobs=3\n\n# the pupdate_parameters(slurm_array_parallelism=maxjobs, mem_gb=2, timeout_min=4, slurm_partition=\"CPU\", cpus_per_task=1)\n\n# execute the job (note the .map_array command that different from the .submit command used above)\njobs = executor.map_array(simple_function_write, a, b, c)  # just a list of jobs\nprint('### all jobs submitted ###')\nprint('the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal')\n\n# wait a little and kill manually the kernel process\ntime.sleep(3)\nos.system('kill ' + str(os.getpid()))"
  },
  {
    "objectID": "Tutorial_Cluster_Part2.html#conclusion",
    "href": "Tutorial_Cluster_Part2.html#conclusion",
    "title": "Part 2: Good practices",
    "section": "Conclusion",
    "text": "Conclusion\nWhether you need several CPUs, and how to set memory and timeout parameters depend on the functions you use.\nIf you are not sure, look in the documentation of your packages or test for a performance improvement as we just did!\nIm"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "Note\n\n\n\nThis tutorial will be improved soon.\nYou can use this template (and Quarto) in multiple ways. Obviously, you can use it on your personal computer(s), but you can also use directly on a shared server like the CRNL cluster (see the dedicated tuto). One of the main advantages of Quarto for collaborative writing is that it is perfectly integrated with Visual Code (other IDEs also have their neat extensions, like JupyterLab, Rstudio or Neovim) and Zotero. Another big advantage is to facilitate a lot the integration of data processing and figure generation with the writing process. It can even use methods to automatically rewrite your statistics in the main text, each time you re-run an analysis. :blush:"
  },
  {
    "objectID": "tutorial.html#introduction",
    "href": "tutorial.html#introduction",
    "title": "Why Quarto?",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial is obviously more complex than just logging in Google and opening a Google Doc, but it guides and explains you each step and it might still be of interest for those who want a more modern and efficient, privacy-conscious workflow, enabling real-time collaborative writing. Carefully implemented, this template might be particularly useful to any researcher who wishes to make a big step forward in collaborative / open science without losing functionality (in fact, they will gain functionality). Let’s not forget that careers and labs last for decades and that good workflows can save you from weeks or months of (cumulative) suboptimal procedures.\n\nLast but not least, it relies entirely on free, open-source software!\n\nMarkdown is at the heart of our approach. It is the markup language that allows extremely quick and reproducible writing. Markdown is mostly defined by its syntax, where different symbols activate different functions. For example, the symbol # can be used to create titles of different size. # Will produce a huge title, whereas #### will produce a much smaller title. Symbols like *, _ , ~ or ^ allow to write bold, italics, subscript or superscript characters. Messaging apps like What’s App also use some of these syntactic shortcuts, but Markdown goes much farther and allows to create citations, URL links, tables, lists, figures, etc using a similar approach. The Markdown website is a good place to get started. Note that Github and many other Web development tools apps use Markdown routinely. Furthermore, Markdown allows formatting using in-line HTML (very potent) and templating like conventional word processors (i.e. choice of fonts, font style, color, etc.) using a .css style file and other methods.\n\nHowever, as efficient as it can be, Markdown alone is not sufficient for academic writing. 1. First, we should install and configure some tool to manage citations and bibliographies. For this, we will use Zotero which has also an excellent support and great extensions. 2. Third, we may want to set up a pipeline to collaborate efficiently using Github and the LiveShare extension of Visual Code in order to allow real-time collaborative writing (the same approach also allows real-time collaborative coding). It is very well maintained by Microsoft and it is free. 3. Along the way, we may install other optional software such as MikTex to facilitate the writing of equation and export to PDF, or Mermaid to write down graphs in Markdown.\n\nIf you open the file used to render the content you’re reading using an IDE (like Visual Code), you’ll find a lot of comments that explain how Markdown was used to write it. You’ll find examples for most markup techniques. If you’ve clone the repository (as indicated in the installation step), you will find the source file at website/tutorial.qmd."
  },
  {
    "objectID": "tutorial.html#citations",
    "href": "tutorial.html#citations",
    "title": "Why Quarto?",
    "section": "Citations",
    "text": "Citations\nThe first thing we want to learn is to cite a paper. There are many ways to do so, but you’ll probably often use the amazing tool provided by Quarto.\nTo activate Quarto’s citation picker, just type Ctrl+Shift+F8. Otherwise, if you are in the Visual Mode, you can use the menu: Insert –&gt; Citation.\nOnce you’ve cited an item, it will appear as an unformatted citation key [@WATSON1953] in the file. Don’t worry, the principle of Markdown is to decouple writing from rendering text. This is why is it so fast and fluid, but also why you may see peculiar markup, even when using the Visual Mode.\nIn principle, Quarto automatically detects your local Zotero database and gives your the possibility of picking your own articles. However, if you are using Quarto through the VScodeSSH methods, it won’t find your Zotero that runs on a different system (your local computer). It is unlikely that you’ll be often using Quarto in this way to write because this connection mode is mostly useful whenever you want to run demanding computations on the server using the VScodeSSH-Jupyter approach. There are ways to go around this limitations, but it is a bit too advanced for now."
  },
  {
    "objectID": "help_ssh_jupyter.html",
    "href": "help_ssh_jupyter.html",
    "title": "Using Interactive Jupyter over SSH",
    "section": "",
    "text": "Visual Code over SSH is great but it is limited in its ability to run interactive Jupyter notebooks unless X11 forwarding is properly configured. Another way to go is to run a persistent Jupyter lab session on your remote SSH server and forward it through a tunnel."
  },
  {
    "objectID": "help_ssh_jupyter.html#extend-your-.bashrc-file",
    "href": "help_ssh_jupyter.html#extend-your-.bashrc-file",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Extend your .bashrc file",
    "text": "Extend your .bashrc file\nTo do so, we need to edit our .bashrc file (in /home/username/.bashrc) and add the following lines at the end.\n# &gt;&gt;&gt; Jupyterlab Remote &gt;&gt;&gt;\nfunction jlremote {\n    echo $(hostname) &gt; ~/.jupyternode.txt\n    cd /crnldata/cophy/\n    XDG_RUNTIME_DIR= jupyter lab --no-browser --port=9753 --ip=$(hostname)\n}\n# &lt;&lt;&lt; Jupyterlab end config &lt;&lt;&lt;\nNote that the –port variable can be changed to your preference.\nThanks to this piece of code, each time a terminal will open, the function jlremote will be added to the path."
  },
  {
    "objectID": "help_ssh_jupyter.html#launch-the-jupyter-session",
    "href": "help_ssh_jupyter.html#launch-the-jupyter-session",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Launch the Jupyter session",
    "text": "Launch the Jupyter session\nMost remote linux servers will offer the opportunity to run tmux, that allows opening a persistent shell session.\nIn your remote terminal, type tmux. The tmux session will open. That is where we wll run our Jupyter notebook.\nThen, type srun --mem=4G --pty bash. This will associate your tmux session with a compute node on the remote server. You can adjust the memory you need for your notebooks.\nThen, type jlremote. This will launch your Jupyter lab session. It might take a bit of time before displaying a message like this one\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\nCopy the token value (after = sign) and keep it somewhere."
  },
  {
    "objectID": "help_ssh_jupyter.html#set-up-the-ssh-tunnel",
    "href": "help_ssh_jupyter.html#set-up-the-ssh-tunnel",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Set up the SSH tunnel",
    "text": "Set up the SSH tunnel\n\nWindows\nOn your local Windows compute, create a .bat file\n@echo off\nsetlocal\n:: the port you've chosen in the function jlremote \nset port=9753\n:: your username on the remote server\nset remote_username=firstname.lastname\n:: your machine IP\nset remote_hostname=10.69.168.62 \n\nfor /f \"tokens=* usebackq\" %%i in (`powershell -command \"ssh %remote_username%@%remote_hostname% 'tail -1 ~/.jupyternode.txt'\"`) do set node=%%i\n\n:: Read the node from the temporary file\nset /p node=&lt;node.txt\n\nset url=http://localhost:%port%\n\n:: Construct and run the SSH command\nset cmd=ssh -CNL 8888:%node%:%port% %remote_username%@%remote_hostname%\necho Running '%cmd%'\n\n:: Delete the temporary file\ndel node.txt\n\n%cmd%\n\nendlocal\nRun (or double click on) the .bat file.\nYou should be able to use your notebook by using the address provided earlier.\nhttp://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nYou may also just use http://localhost:8888/ and add the token manually.\nYou may set a simple password to access the notebook without the token in the future.\n\n\nLinux\nOn your local Windows compute, create a .bat file\nfunction jllocal {\n    port=9753\n    remote_username=USERNAME\n    remote_hostname=HOSTNAME\n    node=$(ssh $remote_username@$remote_hostname 'tail -1 ~/.jupyternode.txt')\n    url=\"http://localhost:$port\"\n    echo \"Opening $url\"\n    open \"$url\"\n    cmd=\"ssh -CNL \"8888\":\"$node\":\"$port\" $remote_username@$remote_hostname\"\n    echo \"Running '$cmd'\"\n    eval \"$cmd\"\n}\nRun (or double click on) the .bat file.\nYou should be able to use your notebook by using the address provided earlier.\nhttp://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nYou may also just use http://localhost:8888/ and add the token manually .\nYou may set a simple password to access the notebook without the token in the future."
  },
  {
    "objectID": "help_ssh_jupyter.html#credits",
    "href": "help_ssh_jupyter.html#credits",
    "title": "Using Interactive Jupyter over SSH",
    "section": "Credits",
    "text": "Credits\nThis tutorial was largely inspired by this one:\nhttps://benjlindsay.com/posts/running-jupyter-lab-remotely/"
  },
  {
    "objectID": "help_quartonosudo.html",
    "href": "help_quartonosudo.html",
    "title": "Quarto without Sudo rights on Linux",
    "section": "",
    "text": "Note\n\n\n\nThis tutorial will be improved soon.\n\n\nFirst, activate a conda environment that has Jupyter.\nHere are the steps to follow. Open a command line and type:\n`wget https://github.com/quarto-dev/quarto-cli/releases/download/v1.4.504/quarto-1.4.504-linux-amd64.tar.gz`\n`mkdir ~/opt`\n`tar -C ~/opt -xvzf quarto-1.4.504-linux-amd64.tar.gz`\n`mkdir ~/bin`\n`ln -s ~/opt/quarto-1.4.504/bin/quarto ~/bin/quarto`\nIf you have an error at this step because the symbolic link exists already, type: rm ~/bin/quarto\n`( echo \"\"; echo 'export PATH=$PATH:~/bin\\n' ; echo \"\" ) &gt;&gt; ~/.profile`\n`source ~/.profile`\nquarto check"
  },
  {
    "objectID": "getstarted.html",
    "href": "getstarted.html",
    "title": "Getting started",
    "section": "",
    "text": "This template is leveraging modern tools that enable reproducible, open and collaborative science. So, to use it, you will have to install a few things that are very broadly used nowadays.\n\nAn integrated development environment. By default, we will use Visual Code) for this tutorial, but you could work also with JupyterLab, Rstudio or Neovim. Other IDEs such as PyCharm or Eclipse are also possible but less recommended.\nA virtual environment manager. We will use Anaconda that greatly facilitates the creation and management of virtual environments based on Python (but not only). If asked about by the installer, make sure to tick Add Anaconda to my PATH environment variable.\nA version-control software. We will use Git and since this repository is on Github, we’ll also use the Github CLI (Github command line interface). You don’t need to install the Github CLI manually (it is done through Anaconda).\nAn open-source scientific and technical publishing system. We will use Quarto to export our productions to various formats. Quarto is a young but very promising and potent framework dedicated to scientific publishing in all its forms (i.e., dashboards, reports, manuscripts, books, websites, presentations). Make sure to download version 1.4 or above. For now, it is only available from the Pre-release tab. Uninstall the previous version beforehand if you had installed one already.\n\nThe installation of these tools should not take long (~5 minutes), since they are all lightweight and that you just have to follow the default installation procedures.\n\nIf you install quarto on Linux without sudo access, you might need to follow this guide"
  },
  {
    "objectID": "getstarted.html#installation",
    "href": "getstarted.html#installation",
    "title": "Getting started",
    "section": "",
    "text": "This template is leveraging modern tools that enable reproducible, open and collaborative science. So, to use it, you will have to install a few things that are very broadly used nowadays.\n\nAn integrated development environment. By default, we will use Visual Code) for this tutorial, but you could work also with JupyterLab, Rstudio or Neovim. Other IDEs such as PyCharm or Eclipse are also possible but less recommended.\nA virtual environment manager. We will use Anaconda that greatly facilitates the creation and management of virtual environments based on Python (but not only). If asked about by the installer, make sure to tick Add Anaconda to my PATH environment variable.\nA version-control software. We will use Git and since this repository is on Github, we’ll also use the Github CLI (Github command line interface). You don’t need to install the Github CLI manually (it is done through Anaconda).\nAn open-source scientific and technical publishing system. We will use Quarto to export our productions to various formats. Quarto is a young but very promising and potent framework dedicated to scientific publishing in all its forms (i.e., dashboards, reports, manuscripts, books, websites, presentations). Make sure to download version 1.4 or above. For now, it is only available from the Pre-release tab. Uninstall the previous version beforehand if you had installed one already.\n\nThe installation of these tools should not take long (~5 minutes), since they are all lightweight and that you just have to follow the default installation procedures.\n\nIf you install quarto on Linux without sudo access, you might need to follow this guide"
  },
  {
    "objectID": "getstarted.html#configuration",
    "href": "getstarted.html#configuration",
    "title": "Getting started",
    "section": "Configuration",
    "text": "Configuration\nThe following steps assume that the 5 tools described above have been successfully installed. They should take 10-15 minutes to complete.\n\nWork from the command line\nTo use the template and the tools above, you will need to work a little bit from a Terminal. If you have Visual Studio Code or any other good IDE, you will be able to open a terminal from inside it.\nBy default, VS code opens a “Powershell” terminal on windows, but it is better to work from a simple command line, so press Ctrl+Shit+P (a little search field will appear at the top), type “terminal select default”, press Enter and select “Command Prompt”.\nIn VS code you can then open the terminal through the top menu (Terminal -&gt; New Terminal). You could also do Ctrl+Shift+P, type “Create New Terminal” and press enter. In general, all actions and settings of VScode can be searched and accessed through Ctrl+Shift+P.\n\n\nLook for a convenient place on your computer\nAny command line interface allows you to cd to any place of your computer. On Windows, if you need to move from one drive to another, you’ll have first to type the letter of your drive (like this M: or D:). If you need to create a dir in the current directory, you can use the function mkdir WhateverNewFolderName.\n\n\nNever use a path containing spaces.\nNever combine systems like Google Drive or Dropbox with version-controlled folder.\nThe shortest the path the better (for example, something like C:/Projects/ is great)\n\n\nFor the rest of this tutorial, we will assume that you work in: C:/Projects/ but you could work in any other folder. So, if we don’t have that folder already we can type the following commands.\nC:\nmkdir Projects\ncd Projects\n\n\nLog in Github and get the template\n\nProper method\n\nGo on Github and login into your account (create one if you do not have one already).\nVisit the following page: https://github.com/cophyteam/project-template and click on the green button Use this template -&gt; Create a new repository. Give the repository any name you want. Hereafter, we will assume that you named it mycophyproject. You can keep it public (if you are ok for anyone to see it) or private.\nOnce it has generated the repository, click on the green button Code and copy paste the address of the repository (e.g. `https://github.com/yourgitname/mycophyproject.git)\nReturn to your command line and type gh auth login (this step requires the Github CLI installed). Follows the instructions to connect your command line with your Github account.\nFinally, type git clone https://github.com/yourgitname/mycophyproject.git.\n\nThis proper method allows you: (i) to use private repositories and (ii) to edit the repositories. Note that Github CLI (gh) is not strictly necessary here, but it helps.\n\n\nQuick and dirty\nJust type git clone https://github.com/cophyteam/project-template. Doing so will immediately get you the template repository. You won’t even need a Github account or the Gitlab CLI, but it means that you will have to configure your Git by yourself (which may be the way to go if you prefer Gitlab over Git for example)\n\n\nQuicker and dirtier\nJust visit https://github.com/cophyteam/project-template, click the gray button Code -&gt; Download Zip and unzip where you want. If you choose this way, you don’t even need to install Git. Although, it might be useless to do this tutorial if you don’t want to use Git.\n\n\n\nFinish the configuration\nAssuming that you entered in the newly created folder already (e.g. by typing cd mycophyproject or cd C:/Projects/mycophyproject), there is just a few commands to run to finalize the installation of the template.\n\ndepending on your system, type conda env create --file=quarto-env-windows.yml or conda env create --file=quarto-env-linux.yml\ntype conda activate quarto-env\ntype python -m ipykernel install --user --name quarto-env\ntype quarto install tinytex (optional, to generate PDF documents)\n\nIf the conda commands throw an error (it should not if you read carefully the installation guidelines above), check this troubleshooting.\nThat’s it! You can now use and modify the template to your needs. Check the corresponding section for more guidance on how to use and develop the template together with Quarto.\nIf you encountered an issue, you can report it here"
  },
  {
    "objectID": "getstarted.html#bonus-steps-for-vscode-users",
    "href": "getstarted.html#bonus-steps-for-vscode-users",
    "title": "Getting started",
    "section": "Bonus steps for VScode users",
    "text": "Bonus steps for VScode users\nOne strength of Visual Studio Code is its amazing ecosystem of extensions. Getting the right extensions can facilitate your life in many ways. Hereafter, a list of extensions you might want to install:\n\nConnect directly to any remote server: SSH extension\nReal-time collaboration tool (great to debug and even write together): LiveShare\nFacilitate the use and the understanding of Git: GitGraph\nPreview any website or HTML within VScode: VScode browser\nUse interactive Jupyter notebooks within VScode: Jupyter\nSimplifies working with Quarto: Quarto\nGet the help of GPT when coding: GenieAI\nMake almost any code look prettier: Prettier\nFacilitate the use of Python: Python\n\nThere are many more to explore (see File -&gt; Extensions)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "COPHY Project",
    "section": "",
    "text": "This website provides information about the template used for new scientific projects in the COPHY team. Like the template itself, the website may change and grow over time."
  },
  {
    "objectID": "about.html#idea",
    "href": "about.html#idea",
    "title": "COPHY Project",
    "section": "Idea",
    "text": "Idea\nAll scientific project share similar components and they can therefore be integrated within a standard template, provided that the template is flexible enough. Templating does not only facilitate collaborative open science, it can also greatly accelerate the overall scientific process, thanks to a myriad of excellent tools such as Git, Quarto, Jupyter, etc."
  },
  {
    "objectID": "about.html#dissociating-writing-from-rendering-content",
    "href": "about.html#dissociating-writing-from-rendering-content",
    "title": "COPHY Project",
    "section": "Dissociating writing from rendering content",
    "text": "Dissociating writing from rendering content\nAcademics spend a lot of their time writing papers, proposals, reports and other documents. Many are happy with traditional workflows combining a commercial word processors (e.g. Word) with a bibliography software such as Zotero or Mendeley. But academic writing is often collaborative and a lot of researchers actually need to use Google Docs (with some plugins) to do so. The problem is that Google Docs offer little guarantees when it comes to privacy, and it is rather limited when it comes to templating, writing equations or simply using it offline. Moreover, Google Docs still constitute one more interface in the daily life of academics (and not the best). By reading step by step the documentation of this website, you may set up an alternative writing workflow using Markdown.\nHabits are strong and starting to use Markdown entails a little switch cost, but it should be largely compensated by all the benefits of using it in the context of the proposed template. Check the documentation of this website to know more."
  },
  {
    "objectID": "about.html#how-to-contribute",
    "href": "about.html#how-to-contribute",
    "title": "COPHY Project",
    "section": "How to contribute",
    "text": "How to contribute\nTo contribute to the development and conception of the template, you can write to romain.ligneul@inserm.fr. You’ll need a Github account. You will also have to join the Cophy Team organization."
  },
  {
    "objectID": "edit_this_website.html",
    "href": "edit_this_website.html",
    "title": "How to edit this website",
    "section": "",
    "text": "To modify this website you’ll have to:\n\nJoin the cophyteam organisation on Github. If you have not already been added, please share your Github account in the dedicated channel on Cophy Mattermost (i.e. ⚒️ Git - Github).\nStill on Mattermost, ask Mathilde or Romain to add you as a collaborator to the project-template repository.\nLog into Github and make sure your command line is connected to (see the Github help if needed)\nClone the project-template repository.\nMake the change you want to do in the website folder.\nMake your you’ve cd in the website-specific folder and type quarto render\n\n\nPlease only edit the .qmd files within the “website” folder. If you want to add a new .qmd web page, then you’ll have to update the file .website/_quarto.yml (website: navbar: zone). In principle, you should not need to changes other file. You may play with the .scss file if you know what you are doing.\n\n\nReturn to the main folder cd ..\nAdd, commit and push to the main branch.\nReturn to the website folder cd website and type quarto publish gh-pages. This should automatically trigger and update of the website after a few minutes (you can check the deployment process here)\n\nSimple, isn’t it?"
  },
  {
    "objectID": "help_github.html",
    "href": "help_github.html",
    "title": "Quick tutorial for Git/Github",
    "section": "",
    "text": "If you only plan to clone public repositories, you can probably just use git clone https:// or simply download and unzip the repository of interest.\nHowever, you will need to connect your command line to your Github account in the following scenarios:\n\nto clone private repositories that you own or one which you have been added as a collaborator.\nto perform any write operation in the repository (e.g. push, pull, merge, branch, etc.)\n\n\nIf you have admin/sudo rights\n\nIf you don’t have Github, create an account.\nInstall the Github CLI utility that will allow you to connect your command line to your Github repository.\nRestart your terminal (or Visual Code), reopen it, and type gh auth login. Choose Github.com, HTTPS, Login with a web browser (or use an auth token if you prefer)\nCD to the directory where your example.md file is located and type git init in the command line. This preconfigures the folder as a Git repository.\nIn Github, create a new repository. For example, name it “example-collaborative-writing”. If you create a public repository, everyone on the web will be able to see the repository and its content. If you create a private repository, only the Github contact that you will manually add to the repository will be able to see it and its content. Most often, you’ll want to create private repositories to work with specific colleagues.\nBack to your command line, type git remote add origin https://github.com/yourGithubUsername/example-collaborative-writing.git git add ., git commit -m \"first commit\", git branch -M main, git push -u origin main. These steps are only needed for the creation of a new repository, then things will be much simpler. Refresh the web page of the newly create Github repository. You should see your example.md file appearing!\n\nNow, each time that you’ll want to start working on the file(s) contained in the folder/repository, you’ll have to type git pull in the command line (make sure that our command line points to the correct folder, if not, cd to it), make your modification and then type: git add ., git commit -m \"any message\", git push to upload your changes. You may even speed up this process further by using this trick.\nThis tutorial may be extended to cover other important aspects of Git. The two most important concepts are “branching” (see here) and “merging” (see here). A final one is “conflict management” (see here), which is relatively easy to deal with in VScode.\n\n\nIf you don’t have admin/sudo rights (e.g. on the cluster)\nIn this case, you can still connect your command line to your Github. The following is a simplified version of the information provided by Github.\n\nIf you don’t have Github, create an account.\nIn the command line of your computer, type ssh-keygen -t ed25519 -C \"your_email@example.com\" and indicate a passphrase (can be simple but not empty). This will create a new SSH key. Replace your_email@example.com by the email address you’ve used when creating your github account (it should work with other valid addresses too).\nThen, add the SSH key to your ssh client by typing: ssh-add ~/.ssh/id_ed25519 (id_ed25519 is a default name: if you specified a name for your key, adjust accordingly).\nStill in the command line, type cat ~/.ssh/id_ed25519.pub (again, adjust name if necessary) and copy paste what comes out.\nFinally, login into your Github account, go to the following address https://github.com/settings/profileand paste what you got from previous step (it should start with ‘ssh-rsa’ and finish with the email address you’ve provided at step 2).\nThat’s it. Now each time you need to connect to Github from your command line, just type ssh -T git@github.com. You’ll be asked the passphrase of your SSH key (as defined at step 2)."
  },
  {
    "objectID": "help_ssh.html",
    "href": "help_ssh.html",
    "title": "Connecting to the CRNL server over SSH in Visual Code",
    "section": "",
    "text": "Visual Code has a great Remote SSH extension that can be used to connect remotely to the CRNL server, without having to constantly enter your password. Except for using Matlab (it may change soon), working on the server from VScode has only advantages, starting with speed (as compared to laggy VNC connections). Since the connection process can be a bit tedious, here are the steps to follow. You have only to follow them once. It is assumed that you use Windows, but it should be similar on other platforms."
  },
  {
    "objectID": "help_ssh.html#set-up-the-vpn-of-inserm-if-you-are-outside-the-crnl-premises",
    "href": "help_ssh.html#set-up-the-vpn-of-inserm-if-you-are-outside-the-crnl-premises",
    "title": "Connecting to the CRNL server over SSH in Visual Code",
    "section": "Set up the VPN of INSERM (if you are outside the CRNL premises)",
    "text": "Set up the VPN of INSERM (if you are outside the CRNL premises)\nTo access the CRNL servers from outside, following this tutorial.\nhttps://wiki.crnl.fr/doku.php?id=wiki:informatique:services:vpn\nYou need your CRNL identifiers to access this page. Essentially, besides installing the software, you have to fill the PDF “fiche” (here) and send it to pe.aguera@inserm.fr. Then, it will be a matter of days before you can access the CRNL infrastructure remotely. You’ll be noticed by email.\nIf you don’t have an INSERM email, you can ask one if you work at CRNL (even if you are affiliated to CNRS or UCBL)."
  },
  {
    "objectID": "help_ssh.html#connect-with-username-and-password",
    "href": "help_ssh.html#connect-with-username-and-password",
    "title": "Connecting to the CRNL server over SSH in Visual Code",
    "section": "Connect with username and password",
    "text": "Connect with username and password\nIn Visual Code, first click on the bottom left icon of the Remote SSH extension.\n\n\nType, connect to host and enter: ssh yourusername@10.69.168.62\nThen, you’ll be asked to enter your password.\nOpen your /home/yourusername folder (it will ask your password a 2nd time)\nCreate a .ssh folder (/home/yourusername/.ssh) and inside that folder, create a file named authorized_keys (without any extension)\nOpen this file\n\nLeave open this Visual Code session that we will call the server session, to open and create the pair of keys."
  },
  {
    "objectID": "help_ssh.html#create-a-pair-of-keys-and-configure-them",
    "href": "help_ssh.html#create-a-pair-of-keys-and-configure-them",
    "title": "Connecting to the CRNL server over SSH in Visual Code",
    "section": "Create a pair of keys and configure them",
    "text": "Create a pair of keys and configure them\n\nOpen VS code a second time on your local computer. We will call this one the local session\nOpen a terminal in this local session.\ncd to the folder of your choice (that’s where the SSH keys will be stored, so there should be no space in the path)\nType ssh-keygen -t ed25519 and chose whatever name you like for the key. You don’t have to set a password. If ssh-keygen is throwing an error, see here.\nTwo files are created, open the file ending with a .pub extension using VS code or any text editor.\nCopy its content and paste it in the authorized_keys file that should be opened in your VS code server session (the one we used first)\nNow, return to your local session and click again on the blue icon of the Remote SSH extension (bottom left).\nClick Connect to host\nClick Configure SSH Hosts\nClick on the line displaying C:\\Users\\UserName\\.ssh\\config and make sure it looks as follow:\n\nHost 10.69.168.62\n  HostName 10.69.168.62\n  ForwardX11 yes\n  User firstname.lastname\n  IdentityFile C:\\pathofyourkeyfile\\yourkeyfile\nOf course, change the User and IdentityFile fields for your own username and path to the key (do not include the .pub extension)\nThat’s it. In principle, the next time you click on Connect to host in Visual Code, you won’t have to enter any password."
  },
  {
    "objectID": "help_zotero.html",
    "href": "help_zotero.html",
    "title": "Optimize Zotero for Markdown citations",
    "section": "",
    "text": "In principle, if you use the citation utility of Quarto, you don’t have to run the following steps as it will automatically detect your local Zotero and create a bibliography file holding all the references you’ll insert in your project. They are only useful if your want to manually enter citation keys in your Markdown.\n\nTo what flawlessly with Markdown, Zotero only needs one extension: Better Bibtext for Zotero to help keep its librairy updated and available for our Visual Studio / Markdown duo. The installation process is a bit unusual as you need to:\n\nDownload an .xpi file from here (you may need to right click and “save the link target”) and install it in Zotero (see below)\nDownload a .lua file (possibly still available here) as recommended in this tutorial (which is not very clear, but fortunately, no need to read it for now). Just place it somewhere simple on your computer (e.g. at C://whateverfoldername/zotero.lua). Avoid to spaces in the path as spaces make path more annoying in general (e.g.don’t put it in G://My Drive/..).\n\nMake sure to complete the six steps below:\n\nDownload the latest version of ZoteroBetterBibTeX (the .xpi file).\nInstall the .xpi file in Zotero (Tools &gt;&gt; Add-ons &gt;&gt; ‘Cogwheel button’ &gt;&gt; Install add-on from file).\nIn Zotero preferences, set the Better BibTeX citation key format to [auth:lower][year].\nSelect all your references in Zotero, right-click and make a BetterBibTeX refresh (“Refresh BibTeX key”).\nCreate an auto-updating library of your citation keys: Zotero &gt;&gt; File &gt;&gt; Export Library. Use “Better BibLaTeX” format. Tick “Keep updated”. Remember the folder where you put the library. Again, avoid having spaces in your path. For example, you might name and place the file so as to get it at C://whateverfoldername/zoteroMyLibrairy.bib\nIn Zotero, set Preferences &gt;&gt; Better BibTeX &gt;&gt; Automatic export &gt;&gt; Automatic export: On change. This keeps the .bib file updated with any new references that you add in Zotero.\n\nEssentially, what we’ve done here is allow Zotero to communicate with Visual Code and Markdown. We have also prepared the process of exporting citations and creating nice bibliographies."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html",
    "href": "Tutorial_Cluster_Part1.html",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "",
    "text": "Since we have a brand-new cluster, it is a good time to improve your way of using it if you had always felt that your workflow was not optimal. This tutorial will help you to set up your personal environment if you use Python."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#install-miniconda-to-manage-your-virtual-environment",
    "href": "Tutorial_Cluster_Part1.html#install-miniconda-to-manage-your-virtual-environment",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Install miniconda to manage your virtual environment",
    "text": "Install miniconda to manage your virtual environment\nInstalling miniconda is not incompatible with using venv later on.\nOpen a terminal and type the following commands:  cd ~ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh source ~/.bashrc\nFollow the instructions and say yes to everything (you make press Ctrl+C once followed by Enter to skip the text faster)\nNB: In JupyterLab, you can open a new terminal at ‘File-&gt;New-&gt;Terminal’. You can then bring this terminal just below your notebook by clicking on its tab and dragging it toward the lower part of the window."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#create-your-first-conda-environment",
    "href": "Tutorial_Cluster_Part1.html#create-your-first-conda-environment",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Create your first conda environment",
    "text": "Create your first conda environment\nStill in the terminal, type:  conda create -n crnlenv python=3.9  conda activate crnlenv\nThe crnlenv virtual environment is active! Everything that will be installed from now on will only be accessible when crnlenv has been activated"
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#make-your-conda-environment-visible-to-jupyter-lab",
    "href": "Tutorial_Cluster_Part1.html#make-your-conda-environment-visible-to-jupyter-lab",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Make your conda environment visible to Jupyter lab",
    "text": "Make your conda environment visible to Jupyter lab\nStill in the terminal type:  conda install ipykernel  python -m ipykernel install --user --name crnlenv --display-name \"Python (crnlenv)\"\nThis commands allow your kernel to be accessed from Jupyter Lab, not only from the command line. If you create more conda environments / kernels, you will also have to run these lines"
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#populate-your-conda-environment-kernel-with-essential-tools",
    "href": "Tutorial_Cluster_Part1.html#populate-your-conda-environment-kernel-with-essential-tools",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Populate your conda environment / kernel with essential tools",
    "text": "Populate your conda environment / kernel with essential tools\nInstall a package that allow to submit your jobs easily from any Jupyter notebook on Slurm conda install submitit\nInstall numpy  conda install numpy\nInstall a memory_profiler pip install memory_profiler -U\nLater on you could install various other tools in your virtual environment, but the priority is to check that you can use the cluster and distribute your jobs.\nNB: if you wonder why install alternatively with conda or pip, the answer is: you can almost always do it with pip but if it works with conda, the package might be “better” installed in some case."
  },
  {
    "objectID": "Tutorial_Cluster_Part1.html#lets-start-computing",
    "href": "Tutorial_Cluster_Part1.html#lets-start-computing",
    "title": "Part 1: A fresh start on the CRNL cluster",
    "section": "Let’s start computing",
    "text": "Let’s start computing\nSince we had , you should be able to see the crnlenv in Jupyterlab if you go in “Kernel-&gt;Change Kernel”.\nSelect it and then restart the kernel (“Kernel-&gt;Restart Kernel”) to continue this tutorial.\nOn the top right of this window, you should see something like “Python (crnlenv)”. It means your notebook is running in the right virtual environment!\nFrom now on, you will execute the code cells below, in order. You can do it either by pressing the play button (at the top of the notebook) or by clicking in the target cell and pressing Shift+Enter.\nYou may also want to check the tutorials of the module submitit used here.\n\n###### Import packages/modules\nimport submitit\n# memory profiler to evaluate how much your jobs demand\nfrom memory_profiler import memory_usage\n# import garbage collector: it is sometimes useful to trigger the garbage collector manually with gc.collect()\nimport gc\n# import other modules\nimport time\n\n\n###### Define a function that should run on the cluster\n\n# this specific function is very dumb and only for demonstration purposes\n# we will just feed it with a number and a string, but we could pass any object to it (filepath, DataFrames, etc.)\n# here, the function only return one argument but it could return several (result result1, result2)\ndef yourFunction(argument1, argument2):\n\n    # print something to the log\n    print('I am running with argument1=' + str(argument1))\n    \n    # sleep for the duration specified by argument1\n    # just to illustrate the parallel processing implemented\n    time.sleep(argument1)\n    \n    # we simply duplicate argument2 as a function of argument1 and return it as our results\n    results=''\n    for i in range(argument1):\n        results=results+'_'+argument2\n    return results\n\n\n# check time and memory usage of your function\n# ideally, try to test it with the input values that will produce the biggest memory consumption\nstart_time = time.time()\nmem_usage=memory_usage((yourFunction, (3,'consumption',)))\nend_time = time.time()\nprint('Maximum memory usage (in MB): %s' % max(mem_usage))\nprint('Maximum memory usage (in GB): %s' % (max(mem_usage)/1000))\nprint('Time taken (in s): %s' % (end_time-start_time))\n\n\n#### Set some environment variables for our jobs\n### for some reason, some default values are set on the cluster, which do not match \n### each other and submitit will complain\nimport os\nos.environ['SLURM_CPUS_PER_TASK'] = '1'\nos.environ['SLURM_TRES_PER_TASK'] = os.environ['SLURM_CPUS_PER_TASK']\n\n\n#### define some array for which each item will be associated with an independent job on the cluster\n#### when you execute this cells, the jobs are sent to the cluster \n\n# here we define an array of numbers: since this array will be used to feed the first argument of yourFunction\n# and that yourFunction waits for as many second as its first argument, the jobs will return in the wrong order\n# (with the output of the second call about 20s after the first one!)\narray_parallel=[1, 20, 2, 5]\n\n# define an additional parameter to be passed to the function\nadditional_parameter='whatever'\n\n# define a variable that will be outside the scope of the function\nvariable_outside_scope = 'something'\n\n# initialize a list in which our returning jobs will be stored\njoblist=[]\n\n# loop over array_parallel\nprint('#### Start submitting jobs #####')\njcount=0\nfor i, value in enumerate(array_parallel):\n    \n  # executor is the submission interface (logs are dumped in the folder)\n  executor = submitit.AutoExecutor(folder=os.getcwd()+'/tuto_logs/')\n  \n  # set memory, timeout in min, and partition for running the job\n  # if you expect your job to be longer or to require more memory: you will need to increase corresponding values\n  # however, note that increase mem_gb too much is an antisocial selfish behavior :)\n  executor.update_parameters(mem_gb=1, timeout_min=5, slurm_partition=\"CPU\")\n  \n  # actually submit the job: note that \"value\" correspond to that of array_parallel in this iteration\n  job = executor.submit(yourFunction, value, additional_parameter)\n  \n  # add info about job submission order\n  job.job_initial_indice=i \n  \n  # print the ID of your job\n  print(\"submit job\" + str(job.job_id))  \n\n  # append the job to the joblist\n  joblist.append(job)\n\n  # increase the job count\n  jcount=jcount+1\n\n\n### now that the loop has ended we check whether any job is already done\nprint('#### Start waiting for jobs to return #####')\nnjobs_finished = sum(job.done() for job in joblist)\n\n# decide whether we clean our job live or not\nclean_jobs_live=False\n\n# create a list to store finished jobs (optional, and depends on whether we need to cleanup job live)\nfinished_list=[]\nfinished_order=[]\n\n### now we will keep looking for a new finished job until all jobs are done:\nnjobs_finished=0\nwhile njobs_finished&lt;jcount:\n  doneIdx=-1\n  for j, job in enumerate(joblist):\n    if job.done():\n      doneIdx=j\n      break\n  if doneIdx&gt;=0:\n    print(str(1+njobs_finished)+' on ' + str(jcount))\n    # report last job finished\n    print(\"last job finished: \" + job.job_id)\n    # obtain result from job\n    job_result=job.result()\n    # do some processing with this job (we could directly do print(job.results() though obviously!)\n    print(job_result)\n    # decide what to do with the finished job object\n    if clean_jobs_live:\n      # delete the job object\n      del job\n      # collect all the garbage immediately to spare memory\n      gc.collect()\n    else:\n      # if we decided to keep the jobs in a list for further processing, add it finished job list \n      finished_list.append(job)\n      finished_order.append(job.job_initial_indice)\n    # increment the count of finished jobs\n    njobs_finished=njobs_finished+1\n    # remove this finished job from the initial joblist\n    joblist.pop(doneIdx)\n    \nprint('#### All jobs completed #####')\n### If we chose to keep our job results for subsequent processing, it will often be crucial to reorder as a function of their initial\n### submission order, rather than their return order (from the cluster). Here we only keep the results of the job\nif clean_jobs_live==False:\n  finished_results = [finished_list[finished_order[i]].result() for i in finished_order]\n  print('Concatenated results obtained by applying yourFunction() to all items in array_parallel:')\n  print(finished_results)"
  }
]